{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices\n",
    "\n",
    "This project involved building an interactive web app, using Shiny for Python, to display house prices in England and Wales, adding a Choropleth layer to visualise the differences by region. The aim was to demonstrate the Shiny software and investigate trends in house prices over time and across different regions of England and Wales. This notebook contains is analysis into the data and a walkthrough of how the app was built, but if you'd just like to view the app then use this link:\n",
    "\n",
    "*[House Prices Visualisation](https://drooney.shinyapps.io/housepricesvisualisation/)*\n",
    "\n",
    "##### About the data\n",
    "\n",
    "For this project I used the Price Paid Data published monthly by the UK government, which tracks property sales in England and Wales submitted to HM Land Registry for registration. The dataset contains single residential properties sold for value since 1995, and since 2013 includes transfers under power of sale/repossessions, buy-to-lets (where identifiable by Mortgage type) and transfers to non-private individuals. This data is also used by property websites such as Zoopla and Rightmove to allow users to track property prices.\n",
    "\n",
    "The Price Paid Data includes information on all residential property sales in England and Wales that are sold for value and are lodged with us for registration. It excludes:\n",
    "\n",
    "- sales that have not been lodged with HM Land Registry\n",
    "- sales that were not for value\n",
    "- transfers, conveyances, assignments or leases at a premium with nominal rent, which are:\n",
    "- ‘Right to buy’ sales at a discount\n",
    "- subject to an existing mortgage\n",
    "- to effect the sale of a share in a property, for example, a transfer between parties on divorce\n",
    "- by way of a gift\n",
    "- under a compulsory purchase order\n",
    "- under a court order\n",
    "- to Trustees appointed under Deed of appointment\n",
    "- Vesting Deeds Transmissions or Assents of more than one property\n",
    "\n",
    "##### Initial planning\n",
    "\n",
    "- What granularity to use? e.g. county, town, postcode\n",
    "    - The dataset contains the postcode of each property, and every postcode has a clearly defined area, which should be available online\n",
    "    - The whole postcode is likely too granular - using the area code or district code should be a good compromise\n",
    "- How to aggregate price paid by area?\n",
    "    - Allow user to choose which statistic to show on the map\n",
    "    - Could also compare these summary statistics between points in time, which would highlight the areas which have seen the greatest changes in price over time\n",
    "    - Will need to look at the distributions of house price to determine the summary statistics that will be suitable\n",
    "\n",
    "##### Preparing the data\n",
    "\n",
    "Loading the CSV file into a MySQL database:\n",
    "\n",
    "~~~~sql\n",
    "DROP DATABASE IF EXISTS `houseprices`;\n",
    "CREATE DATABASE `houseprices`;\n",
    "USE `houseprices`;\n",
    "\n",
    "CREATE TABLE `pricePaid` (\n",
    "`unique_id` VARCHAR(100),\n",
    "`price_paid` DECIMAL,\n",
    "`deed_date` DATE,\n",
    "`postcode` VARCHAR(8),\n",
    "`property_type` VARCHAR(1),\n",
    "`new_build` VARCHAR(1),\n",
    "`estate_type` VARCHAR(1),\n",
    "`saon` VARCHAR(50),\n",
    "`paon` VARCHAR(50),\n",
    "`street` VARCHAR(50),\n",
    "`locality` VARCHAR(50),\n",
    "`town` VARCHAR(50),\n",
    "`district` VARCHAR(50),\n",
    "`county` VARCHAR(50),\n",
    "`transaction_category` VARCHAR(1),\n",
    "`linked_data_uri` VARCHAR(1),\n",
    "PRIMARY KEY (unique_id)\n",
    ");\n",
    "\n",
    "SET GLOBAL local_infile=ON;\n",
    "SET autocommit=0;\n",
    "SET unique_checks=1;\n",
    "SET foreign_key_checks=0;\n",
    "\n",
    "LOAD DATA LOW_PRIORITY \n",
    "LOCAL INFILE 'Path/To/Project/pricepaid.csv'\n",
    "INTO TABLE pricePaid \n",
    "CHARACTER SET armscii8\n",
    "FIELDS TERMINATED BY ','\n",
    "ENCLOSED BY '\"'\n",
    "LINES TERMINATED BY '\\n' \n",
    "(`unique_id`,`price_paid`,`deed_date`,`postcode`,`property_type`,`new_build`,`estate_type`,`saon`,`paon`,`street`,`locality`,`town`,`district`,`county`,`transaction_category`,`linked_data_uri`);\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had originally planned to use the full dataset in my Shiny app, however the full table is ~5GB in size with ~29m rows. For EDA purposes I am taking a sample of the data so that I can easily load the data into memory in Python. Taking a simple random sample of the data would mean that the number of samples from each area would be proportional to the population of that area, so to ensure that each area had an equal number of samples I would use stratified sampling instead.\n",
    "\n",
    "I needed to choose a level of granularity to which to stratify the data. A UK postcode is made up of 2 parts, the outward code (first part) and inward code (second part), separated by a space. The outward code consists of the postcode area (either 1 or 2 letters) followed by the postcode district (usually 1 or 2 digits). For example, in the postcode PO16 7GZ, PO16 is the outward code (or outcode), PO is the area and 16 is the district.\n",
    "\n",
    "OutCode was added as a generated column to the pricePaid table, along with a Year column and a YearBin column.\n",
    "\n",
    "~~~~sql\n",
    "ALTER TABLE pricePaid ADD COLUMN OutCode VARCHAR(4) GENERATED ALWAYS AS substr(postcode, 1, locate(' ', postcode) - 1) STORED;\n",
    "ALTER TABLE pricePaid ADD COLUMN Year INT GENERATED ALWAYS AS year(cast(deed_date as date)) STORED;\n",
    "ALTER TABLE pricePaid ADD COLUMN YearBin VARCHAR(4) GENERATED ALWAYS AS case when (Year < 2005) then '1995 - 2004' when (Year < 2015) then '2005 - 2014' else '2015 +' end STORED;\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an index on Outcode and YearBin to speed up the stratified sample query.\n",
    "\n",
    "~~~~sql\n",
    "CREATE INDEX OutcodeYearBinIndex ON pricepaid (Outcode, YearBin);\n",
    "~~~~\n",
    "\n",
    "Taking a stratified sample of 100 observations for each distinct OutCode and YearBin. This is to be used for EDA.\n",
    "\n",
    "~~~~sql\n",
    "SELECT t.* FROM\n",
    "(SELECT pp.*, ROW_NUMBER() OVER (PARTITION BY OutCode, YearBin ORDER BY RAND()) AS SeqNum\n",
    "FROM pricePaid pp) t\n",
    "WHERE t.SeqNum <= 100\n",
    "INTO LOCAL OUTFILE '/Path/To/Project/pricepaidsample.csv'\n",
    "FIELDS TERMINATED BY ','\n",
    "ENCLOSED BY '\"'\n",
    "LINES TERMINATED BY '\\n';\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploratory data analysis\n",
    "\n",
    "Importing the sample into Python and creating a density plot of all house prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "### Load data\n",
    "appDir = Path(os.path.abspath(''))\n",
    "dataset = pd.read_csv(appDir / \"pricepaidsample.csv\", delimiter='\\t', header=0, encoding=\"utf-8\")\n",
    "# remove missing postcodes and price paid values of zero\n",
    "dataset = dataset[~((dataset['postcode'].isnull()) | (dataset['price_paid'] == 0))]\n",
    "dataset['price_paid_mil'] = dataset['price_paid'] / 1000000\n",
    "\n",
    "# density plot of price paid\n",
    "fig, ax = plt.subplots()\n",
    "sns.kdeplot(df['price_paid_mil'])\n",
    "ax.set_xlabel('Price Paid (£m)')\n",
    "ax.set_xlim(left=-5)\n",
    "ax.set_ylim(bottom=-0.00005)\n",
    "ax.set_title('Density Plot of Price Paid Sample (£m)')\n",
    "ax.tick_params(bottom=True, left=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EDADensityPlot](../images/EDADensityPlot.PNG)\n",
    "\n",
    "This distribution is heavily right skewed due to some houses selling for 100s of millions. Looking at the summary statistics for the dataset indicates that the vast majority of price_paid values are less than £1m, so filtering out values greater than this should give a better view of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![summarystats](../images/summarystats.PNG)\n",
    "\n",
    "Applying this <= £1m filter and also splitting the data by region to produce a ridge plot so that we compare the distributions by region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### postcode area to region lookup\n",
    "outcodeRegion = pd.read_csv(appDir / \"postcode_region_lookup.csv\")\n",
    "outcodeRegion['Area'] = outcodeRegion['Postcode prefix']\n",
    "dataset['Area'] = dataset['Outcode'].replace('\\d+', '', regex=True)\n",
    "df = pd.merge(dataset, outcodeRegion, on=\"Area\", how=\"left\")\n",
    "\n",
    "# filtering data for house prices <= £1m\n",
    "df_filtered = df[df['price_paid_mil'] <= 1]\n",
    "\n",
    "# ridge plot of log price paid by region\n",
    "def ridgePlot(df, xVar, groups):\n",
    "    sns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "    palette = sns.color_palette(\"Set2\", 12)\n",
    "    g = sns.FacetGrid(df, palette=palette, row=groups, hue=groups, aspect=9, height=1.2)\n",
    "    g.map_dataframe(sns.kdeplot, x=xVar, fill=True, alpha=1)\n",
    "    g.map_dataframe(sns.kdeplot, x=xVar, color='black')\n",
    "\n",
    "    def label(x, color, label):\n",
    "        ax = plt.gca()\n",
    "        ax.text(0, .2, label, color='black', fontsize=13,\n",
    "                ha=\"left\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "    g.map(label, groups)\n",
    "    g.figure.subplots_adjust(hspace=-.5)\n",
    "    g.set_titles(\"\")\n",
    "    g.set(yticks=[], xlabel=\"Price Paid £m\", ylabel=\"\")\n",
    "    g.despine(left=True)\n",
    "\n",
    "ridgePlot(df_filtered, \"price_paid_mil\", \"UK region\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EDARidgePlot](../images/EDARidgePlot.PNG)\n",
    "\n",
    "This shows that all regions have the same type of distribution - though some regions, such as London and South East, are more heavily right skewed. Next I am trying to see if I can fit this distribution to a known distribution, specifically the lognormal distribution (a distribution is lognormal if its log follow a normal distribution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "### determining the distribution type\n",
    "shape, location, scale = scipy.stats.lognorm.fit(df_filtered['price_paid_mil'])\n",
    "mu, sigma = np.log(scale), shape\n",
    "xmin, xmax = np.min(df_filtered['price_paid_mil']), np.max(df_filtered['price_paid_mil'])\n",
    "x = np.linspace(xmin, xmax, 1000)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(x, scipy.stats.lognorm.pdf(x, shape, location, scale), label=\"Lognormal\")\n",
    "sns.kdeplot(df_filtered, x=\"price_paid_mil\", label=\"Density Plot\")\n",
    "ax.set_xlabel('Price Paid (£m)')\n",
    "ax.set_ylim(bottom=-0.1)\n",
    "ax.set_title('Price Paid Sample (£m) - Density Plot vs Fitted Lognormal Distribution')\n",
    "ax.tick_params(bottom=True, left=True)\n",
    "plt.legend(facecolor='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EDAFitToLognormal](../images/EDAFitToLognormal.PNG)\n",
    "\n",
    "Except for some of the data points at the peak of the curve, this appears to give a very close fit. We could conclude from this that the overall distribution of house prices less than £1m closely fits a lognormal distribution. We can check this with a QQ plot of the log of house price against a reference normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "df_filtered['log_price_paid_mil'] = np.log(df_filtered['price_paid_mil'])\n",
    "\n",
    "### QQ plot\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "sm.qqplot(data=df_filtered['log_price_paid_mil'], line='s', ax=ax[0])\n",
    "sns.kdeplot(df_filtered['log_price_paid_mil'], ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EDAQQPlot](../images/EDAQQPlot.PNG)\n",
    "\n",
    "The log of price paid closely matches the normal distribution in the centre, but deviates significantly in the tails, suggesting that the data does not actually quite follow a lognormal distribution. Therefore, instead of using the lognormal parameters as summary statistics in my visualisation to describe the data, non-parametric statistics such as the median and interquartile range (IQR) will be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transforming and aggregating the data\n",
    "\n",
    "Before the data can be used in the Shiny app, it needs to be aggregated. Doing this aggregation outside the Shiny app and reading the aggregated data directly instead greatly improves the performance of the app, hence why I'm using SQL below.\n",
    "\n",
    "I'm choosing to use Outcode as the area to aggregate by, since these areas on the map should granular enough to give a detailed spatial distribution on the map, and each outcode should be of a similar size. The summary statistics are chosen taking into consideration the highly right skewed shape of the pseudo-lognormal distribution - the median will be a better estimate of the average value than the arithmetic mean (because it's not affected by outliers), and the IQR will be a better estimate of spread for the same reason. I am also including skew since we know that the distributions of some regions are more heavily skewed than others.\n",
    "\n",
    "We are also aggregating by YearBin so that we will be able to compare the data between different time periods.\n",
    "\n",
    "~~~~sql\n",
    "CREATE TABLE minMaxMeanCountStddev AS\n",
    "SELECT Outcode, YearBin\n",
    ",MIN(price_paid) AS min\n",
    ",MAX(price_paid) AS max\n",
    ",AVG(price_paid) AS mean\n",
    ",STD(price_paid) AS stddev\n",
    ",COUNT(*) AS count\n",
    "FROM pricePaid\n",
    "GROUP BY Outcode, YearBin;\n",
    "\n",
    "CREATE TABLE skew AS\n",
    "SELECT t1.Outcode, t1.YearBin\n",
    ",SUM(POWER((price_paid - t1.mean), 3) / ((t1.count - 1) * POWER(t1.stddev, 3))) as skew\n",
    "FROM minMaxMeanCountStddev t1\n",
    "INNER JOIN pricePaid pp on t1.Outcode = pp.Outcode and t1.YearBin = pp.YearBin\n",
    "WHERE t1.count <> 1 and t1.stddev <> 0 --avoiding division by zero error\n",
    "GROUP BY t1.Outcode, t1.YearBin;\n",
    "\n",
    "--this method of calculating the quartiles will not be entirely accurate if the actual quartile falls between 2 values, but it is close enough for this purpose\n",
    "CREATE TABLE quartiles AS\n",
    "SELECT Outcode, YearBin, QUARTILE, MAX(price_paid) AS Q1_Q2_Q3\n",
    "FROM \n",
    "(SELECT Outcode, YearBin, price_paid, NTILE(4) OVER (PARTITION BY Outcode, YearBin ORDER BY price_paid) AS QUARTILE\n",
    "FROM pricepaid) S\n",
    "WHERE QUARTILE <= 3\n",
    "GROUP BY Outcode, YearBin, QUARTILE;\n",
    "\n",
    "CREATE TABLE quartilesTransposed as \n",
    "SELECT s.Outcode, s.YearBin\n",
    ",SUM(s.lowerQuartile) AS lowerQuartile\n",
    ",SUM(s.median) AS median\n",
    ",SUM(s.upperQuartile) AS upperQuartile\n",
    "FROM\n",
    "(SELECT Outcode, YearBin\n",
    ",CASE WHEN Quartile = 1 THEN Q1_Q2_Q3 ELSE 0 END AS lowerQuartile\n",
    ",CASE WHEN Quartile = 2 THEN Q1_Q2_Q3 ELSE 0 END AS median\n",
    ",CASE WHEN Quartile = 3 THEN Q1_Q2_Q3 ELSE 0 END AS upperQuartile\n",
    "FROM quartiles) s\n",
    "GROUP BY s.Outcode, s.YearBin;\n",
    "\n",
    "SELECT t1.*, t2.median, t2.lowerQuartile, t2.upperQuartile, t3.skew\n",
    ", t1.max - t1.min AS range_, t2.upperQuartile - t2.lowerQuartile AS IQR\n",
    "FROM minMaxMeanCountStddev t1\n",
    "INNER JOIN quartilesTransposed t2 on t1.Outcode = t2.Outcode and t1.YearBin = t2.YearBin\n",
    "INNER JOIN skew t3 on t1.Outcode = t3.Outcode and t1.YearBin = t3.YearBin\n",
    "INTO LOCAL OUTFILE '/Path/To/Project/summary.csv'\n",
    "FIELDS TERMINATED BY ','\n",
    "ENCLOSED BY '\"'\n",
    "LINES TERMINATED BY '\\n';\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the aggregated data into Python and additionally prepare a GeoJSON file, which is needed so that the Shiny app knows where the boundaries of each Outcode area are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "### Load aggregated data\n",
    "appDir = Path(__file__).parent\n",
    "summary = pd.read_csv(appDir / 'app/summary.csv', encoding='utf-8', delimiter=',')\n",
    "summary = summary[(~summary['Outcode'].isnull())]\n",
    "summary = summary.rename(columns={\"range_\": \"range\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A folder containing a GeoJSON file with the polygon coordinates of each Outcode was downloaded online (https://github.com/mhl/postcodes-mapit). This is combined into a single GeoJSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "\n",
    "### Get polygon coordinates (GeoJSON) of each PostcodeArea\n",
    "geojsonDir = appDir / 'districts'\n",
    "# combine all PostcodeArea datasets, one for each PostcodeArea\n",
    "geojsonDict = {}\n",
    "for file in geojsonDir.glob('*.geojson'):\n",
    "    with open(file, 'r') as f:\n",
    "        geojsonDict[str(file).split('\\\\')[-1][:-8]] = json.load(f)\n",
    "# add id string to link to summary data\n",
    "for key in geojsonDict.keys():\n",
    "    geojsonDict[key]['features'][0]['id'] = key\n",
    "# changing format of geojsonDict to meet required format for Choropleth function\n",
    "geojsonList = []\n",
    "uniqueOutcodes = summary['Outcode'].unique().tolist()\n",
    "for outcode in geojsonDict.keys():\n",
    "    features = geojsonDict[outcode]['features']\n",
    "    if outcode in uniqueOutcodes:\n",
    "        geojsonList.append(features[0])\n",
    "geojsonDict = {}\n",
    "geojsonDict['type'] = 'FeatureCollection'\n",
    "geojsonDict['features'] = geojsonList\n",
    "\n",
    "### make sure summary data has a row for every Outcode and YearBin - set aggregate values to null if missing\n",
    "uniqueYearBins = summary['YearBin'].unique().tolist()\n",
    "crossJoin = list(itertools.product(uniqueOutcodes, uniqueYearBins))\n",
    "crossJoin = pd.DataFrame(crossJoin, columns=[\"Outcode\", \"YearBin\"])\n",
    "summary = pd.merge(crossJoin, summary, how=\"left\", on=[\"Outcode\", \"YearBin\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the final step in the data preparation, the GeoJSON file and summary dataset were exported to the folder so that they can be read into the Shiny app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export as geojson dictionary as json file\n",
    "with open(appDir / 'app/OutcodeCoordinates.json', 'w') as fp:\n",
    "    json.dump(geojsonDict, fp)\n",
    "\n",
    "# export summary dataframe as csv\n",
    "summary.to_csv(appDir / 'app/summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building the Shiny app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The app.py program of the Shiny app consists of 3 main parts:\n",
    "\n",
    "- importing the data\n",
    "- building the HTML interface\n",
    "- defining a server function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from ipyleaflet import Map, Choropleth\n",
    "from shiny import App, Inputs, Outputs, Session, ui, reactive, render\n",
    "from shinywidgets import output_widget, render_widget\n",
    "from branca.colormap import linear\n",
    "\n",
    "### Load data\n",
    "appDir = Path(os.path.abspath(''))\n",
    "with open(appDir / 'OutcodeCoordinates.json', 'r') as f:\n",
    "    outcodeCoordinates = json.load(f)\n",
    "outcodes = [i['id'] for i in outcodeCoordinates['features']]\n",
    "summary = pd.read_csv(appDir / 'summary.csv', encoding='utf-8', delimiter=',')\n",
    "summary = summary[(~summary['Outcode'].isnull()) & (summary['Outcode'].isin(outcodes))]\n",
    "summary = summary.rename(columns={\"range_\": \"range\"})\n",
    "yearBins = list(summary['YearBin'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Building the HTML interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nest Python functions to build an HTML interface\n",
    "app_ui = ui.page_fillable( \n",
    "    # Layout the UI with Layout Functions\n",
    "    # Add Inputs with ui.input_*() functions \n",
    "    # Add Outputs with ui.output_*() functions\n",
    "    ui.layout_sidebar(\n",
    "        ui.sidebar(\n",
    "            ui.input_checkbox_group('yearBin', \"Time Period\", yearBins),\n",
    "            ui.input_select('statistic', \"House Price Summary Statistic\", ['mean', 'median', 'min', 'max', 'IQR', 'range', 'skew'], selected='median', multiple=False),\n",
    "            ui.input_switch(\"switch\", \"Compare Summary Statistic between Time Periods\", False)\n",
    "        ),\n",
    "        ui.card(\n",
    "            ui.output_image(\"image\", height='10px'),\n",
    "            output_widget(\"map\", fill=True),\n",
    "            full_screen=True,\n",
    "            fill=True\n",
    "        ),\n",
    "        ui.card_footer(\"Contains HM Land Registry data © Crown copyright and database right 2021. This data is licensed under the Open Government Licence v3.0.\"),\n",
    "        full_screen=True\n",
    "    ),\n",
    "    title=\"House Prices Visualisation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in a HTML template that looks like this:\n",
    "\n",
    "![HTMLTemplate](../images/HTMLTemplate.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Defining a server function\n",
    "\n",
    "Within the server function, the createChoroData function is reactive and will be called every time one of the inputs to the app changes. The functions uses the user inputs to filter the dataset and return a dictionary containing a key for each Outcode and a value corresponding to the summary statistic.\n",
    "\n",
    "The createChoroData function also contains logic for if more than one time period is selected - in this case the function will calculate the difference between the summary statistics of the earliest and latest time period selected.\n",
    "\n",
    "The map function creates an ipyleaflet Map object and adds a Choropleth layer to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define server\n",
    "def server(input: Inputs, output: Outputs, session: Session):\n",
    "    # add colour map image\n",
    "    @render.image\n",
    "    def image():\n",
    "        img = {\"src\": appDir / \"ColourMap.jpg\", \"width\": \"300px\"}  \n",
    "        return img\n",
    "\n",
    "    # function to filter the summary dataset and return a lookup dictionary with a key for each Outcode\n",
    "    @reactive.calc\n",
    "    def createChoroData():\n",
    "\n",
    "        # select all time periods if none are selected\n",
    "        if input.yearBin() == tuple():\n",
    "            filter = yearBins\n",
    "        else:\n",
    "            filter = list(input.yearBin())\n",
    "\n",
    "        # logic for comparing summary stastics between time periods\n",
    "        if input.switch():\n",
    "            minYearBin = filter[0]\n",
    "            maxYearBin = filter[-1]\n",
    "            dfMin = summary[summary['YearBin'] == minYearBin][['Outcode', input.statistic()]]\n",
    "            dfMax = summary[summary['YearBin'] == maxYearBin][['Outcode', input.statistic()]]\n",
    "            df = pd.merge(dfMin, dfMax, how=\"inner\", on=\"Outcode\")\n",
    "            df['diff'] = df[input.statistic() + '_y'] - df[input.statistic() + '_x']\n",
    "            df = df.set_index('Outcode')\n",
    "            df['decile'] = pd.qcut(df['diff'], 10, labels=False)\n",
    "            return df['decile'].to_dict()                \n",
    "        # if not comparing time periods then just show summary statistic\n",
    "        else:\n",
    "            df = summary[summary['YearBin'].isin(filter)].set_index('Outcode')\n",
    "            df['decile'] = pd.qcut(df[input.statistic()], 10, labels=False)\n",
    "            return df['decile'].to_dict()\n",
    "\n",
    "    ### For each output, define a function that generates the output\n",
    "    @render_widget  \n",
    "    def map():\n",
    "        # create a Map object and add a Choropleth layer to it\n",
    "        m = Map(center=(54.00366, -2.547855), zoom=5.5, zoom_snap=0.2, zoom_delta=0.2)\n",
    "\n",
    "        try:\n",
    "            layer = Choropleth(\n",
    "                    geo_data=outcodeCoordinates,\n",
    "                    choro_data=createChoroData(),\n",
    "                    key_on='id',\n",
    "                    colormap=linear.viridis,\n",
    "                    border_color='black',\n",
    "                    style={'fillOpacity': 0.8, 'dashArray': '5, 5'}\n",
    "                )\n",
    "        except:\n",
    "            raise IndexError(\"Must select more than one time period if comparing summary statistic between time periods.\")\n",
    "\n",
    "        m.add(layer)\n",
    "\n",
    "        return m\n",
    "\n",
    "# Call App() to combine app_ui and server() into an interactive app\n",
    "app = App(app_ui, server)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transforming the data (again)\n",
    "\n",
    "The Shiny app is now working, however it is not very insightful because (except for a couple of areas in London) every area appears to be the same colour:\n",
    "\n",
    "<img src=\"../images/MapNoTransformation.PNG\" alt=\"MapNoTransformation\" width=\"20px\" class=\"bg-primary\">\n",
    "\n",
    "The density plot below, of all of the house prices in the sample, shows what the problem is. The data is highly right skewed, and because the colour map works by assigning a different colour to each quantile, the majority of data has the same colour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "### Load data\n",
    "appDir = Path(os.path.abspath('')).parent\n",
    "summary = pd.read_csv(appDir / 'app/summary.csv', encoding='utf-8', delimiter=',')\n",
    "summary = summary[(~summary['Outcode'].isnull())]\n",
    "summary = summary.rename(columns={\"range_\": \"range\"})\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10,12))\n",
    "\n",
    "summary['median_mil'] = summary['median'] / 1000000\n",
    "summary['log_median_mil'] = np.log(summary['median_mil'])\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 12))\n",
    "sns.kdeplot(data=summary, x='median_mil', ax=ax[0])\n",
    "sm.qqplot(summary['median_mil'], ax=ax[1])\n",
    "ax[0].set_xlabel('Price Paid Medians (£m)')\n",
    "ax[0].set_xlim(left=-5)\n",
    "ax[0].set_ylim(bottom=-0.005)\n",
    "ax[0].set_title('Density Plot of Price Paid Sample Medians (£m)')\n",
    "ax[0].tick_params(bottom=True, left=True)\n",
    "ax[1].set_title('QQ Plot of Price Paid Sample Medians (£m)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DensityPlot](../images/DensityPlot.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To resolve this, the QuantileTransformer function from scikit-learn was used to mathematically transform the data to follow a more normal distribution. This is a non-parametric method that works by mapping each quantile of the input distribution onto those of the output distribution (in this case the normal distribution) and applying the required transformation to each quantile so that they match as closely as possible. This is often used to transform features for machine learning problems because certain machine learning algorithms cannot effectively deal with non-normal features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "qt = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "values = qt.fit_transform(np.array(list(summary['median_mil'])).reshape(-1, 1)).flatten()\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 12))\n",
    "sm.qqplot(data=np.array(values), ax=ax[1])\n",
    "sns.kdeplot(data=values, ax=ax[0])\n",
    "ax[0].set_xlabel('Quantile Transformed Price Paid Medians (£m)')\n",
    "ax[0].set_xlim(left=-5)\n",
    "ax[0].set_ylim(bottom=-0.005)\n",
    "ax[0].set_title('Density Plot of Quantile Transformed Price Paid Sample Medians (£m)')\n",
    "ax[0].tick_params(bottom=True, left=True)\n",
    "ax[1].set_title('QQ Plot of Quantile Transformed Price Paid Sample Medians (£m)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DensityPlotQuantileTransformed](../images/DensityPlotQuantileTransformed.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in a distribution that is much more normal, so should make more sense when the colour map is applied to it. However, we can see from the QQ plot that the distribution deviates from normality at each extreme.\n",
    "\n",
    "The following function was added to the Shiny app and applied to the lookup dictionary returned by the createChoroData function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantileTransformer(dictionary):\n",
    "    qt = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "    values = qt.fit_transform(np.array(list(dictionary.values())).reshape(-1, 1)).flatten()\n",
    "    return dict(zip(dictionary.keys(), values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This change to the app produces the map below:\n",
    "\n",
    "![MapQuantileTransformed](../images/MapQuantileTransformed.PNG)\n",
    "\n",
    "This is definitely an improvement, as we can now see some colour variation across the map, however the range of colours is still quite narrow. This is due to the long tails shown in the density plot.\n",
    "\n",
    "Another approach to this problem was to bin the data into deciles, adding the following step to the createChoroData function before returning a dictionary with the decile as the value in each key:value pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['decile'] = pd.qcut(df[summary_statistic], 10, labels=False) # summary_statistic is either input.statistic() or 'diff'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the deciles of the summary statistic gives a much better colour variation across the map, clearly highlighting the regional differences. This method has the added benefit of being conceptually simple, with the top 10% of values being assigned to the brightest colour and bottom 10% being assigned to the darkest colour.\n",
    "\n",
    "![MapDeciles](../images/MapDeciles.PNG)\n",
    "\n",
    "##### GeoJSON compression\n",
    "\n",
    "The app is now working and displaying a useful colour map, however it is extremely slow to respond to any user input. This is because of the large size of the GeoJSON file, which has to be processed every time a change is made to the inputs. An easy work-around to this problem was to use [mapshaper](https://mapshaper.org/) to simplify the GeoJSON file, reducing its size using the Visvalingam / weighted area method with a 1% zoom level. The import was then changed to read from this GeoJSON file instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(appDir / 'OutcodeCoordinates_compressed.json', 'r') as f:\n",
    "    outcodeCoordinates = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis\n",
    "\n",
    "**Regional distribution of median house price**\n",
    "\n",
    "With all time periods combined, the lowest medians are found in South Wales, Birmingham, horizontal strip near Manchester and Leeds, and Tyne and Wear. There is a large area of yellow centred around London, which spreads all across the South. Interestingly, if just filtering for one of the time periods, then we see that the yellow area around London is smaller and that in general areas outside London are darker. This indicates that if we look at a smaller time period of 10 years then there is a greater disparity between London and the rest of the country than if we look at the whole 30 year period, showing that house prices are increasing most rapidly in London and its immediate surroundings.\n",
    "\n",
    "**Which areas have had the greatest change in median over time?**\n",
    "\n",
    "Comparing the change in median across the map between the oldest and most recent time period, London and its surrounding area has clearly had the largest increase in average house price. What is intriguing is the fact that the map in general is much brighter when looking at the overall median, compared to looking at the change in median. This trend is telling us that over a 30 year period most areas have a similar median, however London and its surrounding area has had the greatest change in median over that period. This seems to be contradictory, but makes sense if we assume that the most rapid increase in London house prices has been in the last decade - in that case it is probable that at least 50% of the houses sold were sold before that period, making the median unaffected by the recent change.\n",
    "\n",
    "**Have the house prices in any regions become more skewed over time?**\n",
    "\n",
    "We can see that from 1995 - 2004, areas in the South generally had a more skewed distribution than areas in the North and Wales. However, from 2015+, the split is much less clear, with many of the areas with the most skewed distributions being located in the Midlands and the North. Strangely, these are also the areas that have some of the lowest median house prices. The patches of yellow in the 2015+ view are particularly concentrated around Leicester, Manchester and Sheffield. This change in trend over time suggests that in these areas in the North the gap between the average and most expensive houses has widened, whereas in the South that gap has narrowed. This could either be due to the average in the South shifting upwards, or due to the upper percentile in the other areas shifting upwards. Comparing the change in skew over time indicates that it is probably the latter that is having more of an effect. This could be explained by recent initiatives to reduce the North-South divide, with financial services and tech businesses establishing offices in Leeds and Manchester, which attracts individuals with higher salaries who can afford more expensive homes.\n",
    "\n",
    "**How has the spread of house prices changed over time?**\n",
    "\n",
    "The IQR shows the same regional distribution within each time period, with darker areas in Wales, the North and the Midlands (very similar to the regional distribution of the median); however the difference in colour is less pronounced from 2015+. Despite this, it is still London and its surrounding area that has had the greatest change in IQR over time. This illustrates how in London not only the average price is increasing, the spread of prices is increasing, even if outliers are ignored. \n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "It is no surprise that the average house price is higher in the South of England compared the North, however this visualisation demonstrates that the North-South divide might not be so simple, at least when viewed through the lens of house prices. The housing market is complex, with prices being affected by many different factors, but in general, areas with better employment opportunities, amenities and transportation will have higher house prices because they are more desirable places to live. The median house price distribution by region shows that the socio-economic divide is not as clear as North vs South. The lowest values are concentrated around Wales, the strip West to East near Manchester and Leeds, and Tyne and Wear. However, some areas in the North have higher values, such as south Manchester, North Yorkshire and parts of Cumbria. Looking at the distribution of skew by region shows an even less clear divide between North and South, with the skew from 2015+ being particulary high in many areas in the North and the Midlands. The skew of the house price distribution could be seen as a measure of inequality or wealth disparity, showing that perhaps the wealth gap is now higher in certain areas of the North.\n",
    "\n",
    "Looking at the trends in average (median) house price, we have found that the average has increased across England and Wales but much more rapidly in London and its surrounding area. It is particularly in the last decade that the average house price in London has seen a large spike. Furthermore, the majority of London house prices are now falling within a range that has increased massively compared to the rest of the country."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
